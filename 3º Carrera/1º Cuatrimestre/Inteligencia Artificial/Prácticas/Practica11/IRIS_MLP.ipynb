{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZMx70Qs1paP"
      },
      "source": [
        "# 0. Importaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGMb8hek3DBd"
      },
      "source": [
        "A continuación se importan diversas librerías útiles para el manejo de los datos y la visualización de los resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4F-aa2rlptA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split   #permite dividir conjuntos de datos en train y test\n",
        "from pathlib import Path\n",
        "import imageio.v2 as imageio\n",
        "from IPython.display import Image, display\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAGpQY0pxC4E"
      },
      "outputs": [],
      "source": [
        "sns.set() # Establece el estilo de las gráficas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVhCZMrZeiRn"
      },
      "source": [
        "Específicas de Pytorch se necesita:\n",
        "\n",
        "**torch.nn:** para crear el modelo de red neuronal.\n",
        "\n",
        "**torch.autograd:** se necesita el módulo Variable, que se encarga de manejar las operaciones de los tensores.\n",
        "\n",
        "**torch.optim:**  se necesita el optimizador para entrenar la red neuronal y modificar sus pesos.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM4e_MZtehpK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX207D5NxC4K"
      },
      "source": [
        "# 1. Ejemplo de la clasificación para el problema XOR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwF5NOx8xC4K"
      },
      "source": [
        "Para poder definir una Red Neuronal Artificial(_MLP_ por sus siglas en inglés) haremos uso de la librería Pytorch. Esta librería permite dar nuevas funcionalidades a una clase estándar de Python (tal y como ocurría con las dataclasses). En concreto nos interesa definir dos funciones para esta clase:\n",
        "\n",
        "*   *init*: Se definen las capas de las que consta el modelo,\n",
        "*   *forward*: Se especifica el proceso de la propagación hacia adelante, desde la entrada hasta la capa de salida.\n",
        "\n",
        "Veamos un pequeño ejemplo con el Perceptron para el problema de la separación del XOR. En primer lugar definiremos y mostraremos los datos, para lo cuál definimos la siguiente función."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSI-Guq9Pih9"
      },
      "outputs": [],
      "source": [
        "def mostrar(datos, eje_x, eje_y, nombre_clases=[]):\n",
        "    \"\"\"Muestra una gráfica de puntos utilizando para ello los datos de la\n",
        "    características que se solicitan como parámetro.\n",
        "\n",
        "    Nota: El parámetro hue nos sirve para cambiar el aspecto de la gráfica en función del\n",
        "    # valor que toma en una columna en concreto.\n",
        "    \"\"\"\n",
        "    if len(nombre_clases) == len(datos[\"target\"].unique()):\n",
        "      sns.scatterplot(data=datos, x=eje_x, y=eje_y, hue=nombre_clases[datos[\"target\"]], palette=\"tab10\")\n",
        "    else:\n",
        "      sns.scatterplot(data=datos, x=eje_x, y=eje_y, hue=datos[\"target\"], palette=\"tab10\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ki5U3PxbxC4K"
      },
      "outputs": [],
      "source": [
        "xor = pd.DataFrame({\n",
        "    \"X\": [0.,0., 1., 1.],\n",
        "    \"y\": [0.,1.,0.,1.],\n",
        "    \"target\": [0, 1, 1, 0]\n",
        "})\n",
        "\n",
        "mostrar(xor, \"X\", \"y\")\n",
        "entrada = torch.tensor(xor[[\"X\", \"y\"]].values, dtype=torch.float32) # .values nos da la representación de numpy\n",
        "salida = torch.tensor(xor[\"target\"].values, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iez87ICQxC4L"
      },
      "outputs": [],
      "source": [
        "class Perceptron(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        En init suele añadirse el número de características que tiene el dataset,\n",
        "        en este caso concreto, al ser el problema del xor, utilizamos 2 por defecto\n",
        "        \"\"\"\n",
        "        super(Perceptron, self).__init__()\n",
        "        self.capa1 = nn.Linear(2, 2) # Importante, el 2 inicial representa el nº de características de nuestro dataset. Esta capa se corresponde con la de la entrada.\n",
        "\n",
        "    def forward(self, entrada):\n",
        "        salida = self.capa1(entrada)\n",
        "        probabilidades = F.softmax(salida, dim=1)\n",
        "        return probabilidades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA1-Y4Mx1yIG"
      },
      "source": [
        "Veamos visualmente qué aspecto tiene la red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_Ju1WXU1yIH"
      },
      "outputs": [],
      "source": [
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame(src=f\"https://drive.google.com/file/d/1hOqOdnk6lQphQgHdJA6bksVWzQOgOeb5/preview\", width=640, height=480)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B47ARryBxC4L"
      },
      "source": [
        "Como puede observar en la función **init** de la clase **Perceptron** tenemos diferenciados dos atributos, *capa1* y *soft*. El primero de ellos permitirá realizar una transformación lineal de los datos de entrada. El segundo resulta de gran utilidad, ya que transforma la salida a valores que están en el intervalo $[0,1]$. De este modo, la suma de todos los valores será *exactamente* $1$. Esto es así debido a que estamos representando probabilidades. Luego, en cada índice tendremos valores que representan la probabilidad de que pertenezca a la $i-$ésima clase.\n",
        "\n",
        "Tras esto, podemos definir el resto de elementos necesarios. El optimizador, la función de pérdida y nuestro bucle para realizar el entrenamiento del modelo. Recuerda que para la función de pérdida ($loss$), depende en gran medida del tipo de tarea de queremos que realice la red. Por ejemplo, para realizar una regresión podemos utilizar el error cuadrático medio ($MSE$), mientras que para la clasificación suele usarse la $CrossEntropyLoss$. Por el momento, familiarícese sólo con su uso, sin entrar en más detalle.\n",
        "\n",
        "En cuanto al optimizador, se utilizará el Descenso de Gradiente Estocástico (SGD), con una tasa de aprendizaje de $0.01$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDTNZSuvxC4L"
      },
      "outputs": [],
      "source": [
        "def train(modelo, entrada, salida, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        # 1. Reseteamos el gradiente\n",
        "        optimizador.zero_grad()\n",
        "        # 2. Obtenemos la salida del modelo. Propagación hacia adelante\n",
        "        prediccion = modelo(entrada)\n",
        "        # 3. Obtención del error\n",
        "        error = f_perdida(prediccion, salida)\n",
        "        # 4. Propagamos el error hacia atrás, actualizando los pesos\n",
        "        error.backward()\n",
        "        optimizador.step()\n",
        "    return modelo\n",
        "\n",
        "perceptron = Perceptron()\n",
        "f_perdida = nn.CrossEntropyLoss()\n",
        "optimizador = optim.SGD(perceptron.parameters(), lr=0.01) # En lr especificamos la tasa de aprendizaje (η).\n",
        "epochs = 1000\n",
        "train(perceptron, entrada, salida, epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtRsfg0PxC4L"
      },
      "source": [
        "Si queremos obtener la predicción del modelo para un dato concreto, podemos hacer una llamada al modelo como si fuese una función. Sin embargo, verá que no es muy descriptiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLGKYUyJxC4L"
      },
      "outputs": [],
      "source": [
        "y_hat = perceptron(torch.Tensor([[1., 1.]]))\n",
        "print(y_hat)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gij15a92bKBv"
      },
      "source": [
        "Para devolver la clase 0 o 1, se usa la función argmax, que devuelve la posición del máximo. Dicho de otro modo, la clase **más probable**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSD08GQGbDtP"
      },
      "outputs": [],
      "source": [
        "print(torch.argmax(y_hat))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6V5KPM3xC4M"
      },
      "source": [
        "¿Qué valor predice para el punto $(1, 1)$. ¿Sería correcto? Pruebe con algún punto más."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lc4XbHhST_Bm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDLIr_aa1yIH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YidjTEyIT_LG"
      },
      "source": [
        "Ahora bien, recuerde que el Perceptron básico no nos permite clasificar clases que no sean linealmente separables. Es el caso del problema del XOR. Para visualizar qué está pasando internamente con la red, vamos a crear un par de funciones que ayuden a la visualización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gTf9LymgxC4M"
      },
      "outputs": [],
      "source": [
        "def plot_decision_boundary(model, X, y, epoch):\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01),\n",
        "                         np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        Z = model(torch.from_numpy(np.c_[xx.ravel(), yy.ravel()]).float()).detach().numpy()\n",
        "        Z = torch.softmax(torch.from_numpy(Z), dim=1).numpy()[:, 1]\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    contour = plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.Spectral)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=40, edgecolor='k', cmap=plt.cm.Spectral)\n",
        "    cbar = plt.colorbar(contour)\n",
        "    cbar.set_label('Probabilidad de la clase #1')\n",
        "\n",
        "    plt.title(f\"Epoch {epoch}\")\n",
        "    plt.savefig(f'./imgs/frontera_decision_{epoch}.png')\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwSt84rIxC4M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaytY-6wxC4M"
      },
      "source": [
        "Vamos a crear un _gif_, para lo cual, en primer lugar vamos a crear el directorio en el que vamos a crear las imágenes. Para ver las imágenes, puede hacer click en el icono de la carpeta y ahí verá las imágenes según se irán generando."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp0e9Mv-xC4M"
      },
      "outputs": [],
      "source": [
        "directorio = Path(\"imgs\")\n",
        "directorio.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbnYLBIUxC4V"
      },
      "source": [
        "Y tras esto, vamos a modificar levemente el bucle del entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1B3SI-xgxC4W"
      },
      "outputs": [],
      "source": [
        "def train(modelo, entrada, salida, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        # 1. Reseteamos el gradiente\n",
        "        optimizador.zero_grad()\n",
        "        # 2. Obtenemos la salida del modelo. Propagación hacia adelante\n",
        "        prediccion = modelo(entrada)\n",
        "        # 3. Obtención del error\n",
        "        error = f_perdida(prediccion, salida)\n",
        "        # 4. Propagamos el error hacia atrás, actualizando los pesos\n",
        "        error.backward()\n",
        "        optimizador.step()\n",
        "        if epoch % 10 == 0: # Sólo vamos a guardar una imagen cada 10 épocas\n",
        "            plot_decision_boundary(modelo, entrada, salida, epoch)\n",
        "    return modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UiMTNN5xC4W"
      },
      "source": [
        "Entrenemos el modelo y obtengamos las imágenes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkMKfdXkxC4W"
      },
      "outputs": [],
      "source": [
        "perceptron = Perceptron()\n",
        "f_perdida = nn.CrossEntropyLoss()\n",
        "optimizador = optim.SGD(perceptron.parameters(), lr=0.01) # En lr especificamos la tasa de aprendizaje.\n",
        "epochs = 1000\n",
        "train(perceptron, entrada, salida, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgMSLERDxC4W"
      },
      "outputs": [],
      "source": [
        "imagenes = []\n",
        "for epoch in tqdm(range(1000)):\n",
        "    if epoch % 10 == 0:\n",
        "        imagenes.append(imageio.imread(f'./imgs/frontera_decision_{epoch}.png'))\n",
        "\n",
        "imageio.mimsave('./imgs/frontera_decision.gif', imagenes, fps=15, loop=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-M9bGYvyOX6"
      },
      "outputs": [],
      "source": [
        "imagen = Image(\"imgs/frontera_decision.gif\")\n",
        "display(imagen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0artVugxC4X"
      },
      "source": [
        "¿Qué ocurre si en lugar de conectar la entrada con la salida, agregamos una capa adicional (denominada capa oculta), e incorporamos una función de activación como ReLU?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSGL1sYlxC4X"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        En init suele añadirse el número de características que tiene el dataset,\n",
        "        en este caso concreto, al ser el problema del xor, utilizamos 2 por defecto\n",
        "        \"\"\"\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.capa1 = nn.Linear(2, 8) # Importante, el 2 inicial representa el nº de características de nuestro dataset\n",
        "        self.activacion = nn.ReLU()\n",
        "        self.capa2 = nn.Linear(8, 2)\n",
        "\n",
        "    def forward(self, entrada):\n",
        "        salida_capa1 = self.activacion(self.capa1(entrada))\n",
        "        salida = self.capa2(salida_capa1)\n",
        "        return salida\n",
        "\n",
        "red = MLP()\n",
        "f_perdida = nn.CrossEntropyLoss()\n",
        "optimizador = optim.SGD(red.parameters(), lr=0.01, momentum=0.9) # Momentum es un parámetro para que converja más rápido.\n",
        "epochs = 1000\n",
        "train(red, entrada, salida, 1000)\n",
        "\n",
        "imagenes = []\n",
        "for epoch in tqdm(range(1000)):\n",
        "    if epoch % 10 == 0:\n",
        "        imagenes.append(imageio.imread(f'./imgs/frontera_decision_{epoch}.png'))\n",
        "\n",
        "imageio.mimsave('./imgs/frontera_decision2.gif', imagenes, fps=15, loop=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haZ_25eqyZc2"
      },
      "outputs": [],
      "source": [
        "imagen = Image(\"imgs/frontera_decision2.gif\")\n",
        "display(imagen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghw4ubH3xC4X"
      },
      "source": [
        "Pruebe con otras funciones de activación en lugar de ReLU y observe qué ocurre."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOXtl_1ZxC4X"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        En init suele añadirse el número de características que tiene el dataset,\n",
        "        en este caso concreto, al ser el problema del xor, utilizamos 2 por defecto\n",
        "        \"\"\"\n",
        "        super(MLP, self).__init__()\n",
        "\n",
        "        self.capa1 = nn.Linear(2, 8) # Importante, el 2 inicial representa el nº de características de nuestro dataset\n",
        "        self.activacion = # Incluya otra función de activación Tanh, Sigmoid, ...\n",
        "        self.capa2 = nn.Linear(8, 2)\n",
        "\n",
        "    def forward(self, entrada):\n",
        "        salida_capa1 = self.ReLU(self.capa1(entrada))\n",
        "        salida = self.capa2(salida_capa1)\n",
        "        return salida\n",
        "\n",
        "red = MLP()\n",
        "f_perdida = nn.CrossEntropyLoss()\n",
        "optimizador = optim.SGD(red.parameters(), lr=0.01, momentum=0.9) # Momentum es un parámetro para que converja más rápido.\n",
        "epochs = 1000\n",
        "train(red, entrada, salida, 1000)\n",
        "\n",
        "imagenes = []\n",
        "for epoch in tqdm(range(1000)):\n",
        "    if epoch % 10 == 0:\n",
        "        imagenes.append(imageio.imread(f'./imgs/frontera_decision_{epoch}.png'))\n",
        "\n",
        "imageio.mimsave('./imgs/frontera_decision3.gif', imagenes, fps=15, loop=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bxae--pz8Tl"
      },
      "outputs": [],
      "source": [
        "imagen = Image(\"imgs/frontera_decision3.gif\")\n",
        "display(imagen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpqv-AV-GAAn"
      },
      "source": [
        "### Pregunta 1:\n",
        "¿Tiene sentido cambiar el número de unidades en la capa de salida?\n",
        "¿Y en la capa oculta?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MkVWMqgxC4Y"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeA75FLhfEyf"
      },
      "source": [
        "# 2. Clasificación para la base de datos Iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eylXQ1CcooOP"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "iris = load_iris(as_frame=True) # Vamos a cargarlo como un DataFrame de la librería Pandas\n",
        "X = iris['data']\n",
        "y = iris['target']\n",
        "names = iris['target_names']\n",
        "feature_names = iris['feature_names']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60U4AlPXxC4F"
      },
      "source": [
        "Ahora unamos las entradas y salidas en un único dataset llamado __iris__ y exploremos qué forma tiene el dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hv6_vmnLxC4G"
      },
      "outputs": [],
      "source": [
        "iris = pd.concat([X, y], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WC5fxWmixC4G"
      },
      "outputs": [],
      "source": [
        "iris"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LgdOESAxC4H"
      },
      "source": [
        "¿Qué rango de valores tiene este dataset? Para ello, podemos usar la función _describe_ que nos da información muy útil que podríamos utilizar a posteriori."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pjrca1XFxC4H"
      },
      "outputs": [],
      "source": [
        "iris.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmU8zPqOxC4I"
      },
      "outputs": [],
      "source": [
        "iris.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gINnV6IhxC4I"
      },
      "source": [
        "También es útil ver a nivel gráfico qué forma tienen los datos. Para ello, podemos utilizar la librería seaborn, la cual está orientada a hacer gráficas de forma sencilla usando DataFrames de Pandas. Veamos un pequeño ejemplo con las dos primeras características de Iris: la longitud y el ancho del sépalo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84ARAPdhxC4I"
      },
      "outputs": [],
      "source": [
        "mostrar(iris, \"sepal length (cm)\", \"sepal width (cm)\", names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj66Jz0zxC4I"
      },
      "source": [
        "¿Cómo mostraríamos las dos últimas características, el alto y el ancho del pétalo?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlzYTEp-xC4J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxGmm9eA1yIM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUfY9qwd1yIN"
      },
      "source": [
        "También puede comprobar cómo se distribuyen los valores teniendo en cuenta cada par de características"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ8g8lCK1yIN"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(iris, hue=\"target\", palette=\"tab10\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_knGceExC4J"
      },
      "source": [
        "## 2.1 Preprocesado\n",
        "Cuando estamos trabajando con modelos de Aprendizaje Automático (_Machine Learning_) casi siempre es recomendable realizar algún tipo de preprocesamiento. Uno de los más habituales es realizar un escalado para que los datos estén en una distribución normal, es decir, con media ($\\mu$) 0 y varianza ($\\sigma$) 1. Para ello se sigue la siguiente fórmula:\n",
        "\n",
        "$$ X = \\frac{X - X_{media}}{X_{desviacion\\_tipica}} $$\n",
        "\n",
        "Es decir, a cada elemento del dataset se le resta la media del conjunto completo y tras esto, se divide por la desviación típica. Teniendo en cuenta que las aplicaciones aritméticas se aplican sobre el dataframe completo, hacemos la siguiente transformación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdLbodhkxC4J"
      },
      "outputs": [],
      "source": [
        "iris_standard = iris.copy()\n",
        "iris_standard[iris.columns[:-1]] = (iris_standard[iris.columns[:-1]] - iris_standard[iris.columns[:-1]].mean()) / iris_standard[iris.columns[:-1]].std()\n",
        "iris_standard.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Tn0vKSjHHzx"
      },
      "source": [
        "Importante, como ha podido comprobar no hemos modificado la variable de salida. Sólo las variables de entrada, de ahí el uso de iris.columns[:-1]. Esto devuelve el nombre de todas las columnas menos el de la última."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GScfxknxC4J"
      },
      "source": [
        "Muestre los datos como en el ejemplo anterior. ¿Aprecia alguna diferencia?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3Xum_ylxC4J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3A4UpbPKxC4J"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpaDYzYUxC4K"
      },
      "source": [
        "Otro tipo de preprocesado alternativo y habitual es realizar un escalado para que todos los datos se encuentren en un intervalo $[0,1]$. Para realizar este preprocesado se realiza la siguiente operación.\n",
        "\n",
        "$$ X = \\frac{X - X_{min}}{X_{max} - X_{min}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WbWTYVNxC4K"
      },
      "outputs": [],
      "source": [
        "iris_01 = iris.copy()\n",
        "iris_01[iris.columns[:-1]] = (iris_01[iris.columns[:-1]] - iris_01[iris.columns[:-1]].min()) /  \\\n",
        "                             (iris_01[iris.columns[:-1]].max() - iris_01[iris.columns[:-1]].min())\n",
        "iris_01.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKoXQCo7Y6PC"
      },
      "source": [
        "Muestre los datos como en el ejemplo anterior. ¿Aprecia alguna diferencia?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omE9409VxC4K"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3Q6zqHgHfpv"
      },
      "source": [
        "Esta es una demostración de distintas formas de preprocesar los datos de entrada. Normalmente, no se trata con dataframes de pandas en todo momento, si no que se opta por trabajar con la matriz directamente, ya que resulta más cómodo.\n",
        "\n",
        "Toda esta exploración inicial de los datos, suele formar parte de un proceso más amplío que se sigue en proyectos de este ámbito conocido como Análisis Exploratorio de Datos (*EDA* por sus siglas en inglés). Así, se pueden tener \"pistas\" que ayuden o guíen en la resolución del problema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6NLlDNe2Mwc"
      },
      "outputs": [],
      "source": [
        "iris = iris.values\n",
        "iris_scaled = iris.copy() # Obtenemos la matriz de numpy a partir del DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCjmC_QLJk2f"
      },
      "outputs": [],
      "source": [
        "iris_scaled[:, :-1] = torch.Tensor((iris_scaled[:, :-1] - iris_scaled[:, :-1].max()) / (iris_scaled[:, :-1].max() - iris_scaled[:, :-1].min()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2O-kVbUJ-cP"
      },
      "outputs": [],
      "source": [
        "print(iris_scaled[0:10]) # Imprimimos las 10 primeras filas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frz1Z4Oi98QN"
      },
      "source": [
        "# 3. Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdKELHWjnTDK"
      },
      "source": [
        "Una vez familiarizado con cómo funciona Pytorch, considere la siguiente definición de red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8KpfFs8prxo"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, Nin):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(Nin, 50)\n",
        "        self.layer2 = nn.Linear(50, 50)\n",
        "        self.layer3 = nn.Linear(50, 3)\n",
        "        self.Relu = nn.ReLU()\n",
        "        self.Soft = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.Relu(self.layer1(x))\n",
        "        x = self.Relu(self.layer2(x))\n",
        "        probabilidades = F.softmax(self.layer3(x))\n",
        "        return probabilidades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJRekx071yIN"
      },
      "source": [
        "Y su arquitectura visualmente es la siguiente, siendo $p$ el número de variables y $Cl$ el número de clases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqmVwg761yIO"
      },
      "outputs": [],
      "source": [
        "IFrame(src=f\"https://drive.google.com/file/d/1i6RGJae5yhF6hlWb6Ws-W_NptN8VZKHj/preview\", width=640, height=480)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0XxCi8yoMnD"
      },
      "source": [
        "Hasta ahora, hemos usado tanto un Perceptron como una Red Neuronal Multicapa. Sin embargo, no hemos hecho una división del dataset en conjuntos de entrenamiento y test, ¿verdad? Este proceso es vital para saber la capacidad de generalización de nuestro modelo. De nada sirve obtener un error muy bajo durante el entrenamiento si luego es incapaz de predecir de forma correcta a partir de nuevos datos de entrada. Veamos visualmente la arquitectura de la red."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySMgL_rXxC4Z"
      },
      "outputs": [],
      "source": [
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(\n",
        "    torch.Tensor(iris_scaled[:,:-1]), torch.Tensor(iris_scaled[:,-1]), test_size=0.2, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd5V6pmpoOin"
      },
      "outputs": [],
      "source": [
        "model = MLP(Xtrain.shape[1])\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Rq-oBoetQw0"
      },
      "source": [
        "\n",
        "El proceso de entrenamiento consiste en presentar a la red un número dado de veces ($num_{epochs}$) los datos de entrenamiento para que esta vaya ajustando sus pesos.\n",
        "\n",
        "Después de cada ciclo de entrenamiento se evalúa la salida predicha por el modelo, es decir, se calcula el error cometido entre la predicción y la clase real conocida, tanto con los datos de entrenamiento como con los datos de test. Para ello, volvamos a definir otra función train, pero que contemple los dos nuevos conjuntos. En su forma tradicional, el algoritmo de descenso de gradiente estocástico (SGD) toma el conjunto de datos en pequeños lotes (pudiendo ser una instancia cada vez). En esta ocasión, por simplicidad del código y dado que el conjunto de datos no es muy grande, pasaremos el conjunto de datos de entrenamiento al completo en cada iteración.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5M0it-rZS5n"
      },
      "outputs": [],
      "source": [
        "def train(modelo, X_train, y_train, X_test, y_test, epochs):\n",
        "  lossTrain_list  = np.zeros((num_epochs,))\n",
        "  accTrain_list = np.zeros((num_epochs,))\n",
        "  lossVal_list  = np.zeros((num_epochs,))\n",
        "  accVal_list = np.zeros((num_epochs,))\n",
        "\n",
        "  for epoch in tqdm.trange(num_epochs):\n",
        "      correct=0\n",
        "      Y_pred = model(X_train)\n",
        "      lossTrain = loss_fn(Y_pred, Y_train)\n",
        "      lossTrain_list[epoch] = lossTrain.item()\n",
        "      correct = (torch.argmax(Y_pred, dim=1) == Y_train).type(torch.FloatTensor)\n",
        "      accTrain_list[epoch] = correct.mean() # media de predicciones correctas por epoch\n",
        "\n",
        "      # Zero gradients\n",
        "      optimizer.zero_grad()\n",
        "      lossTrain.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      with torch.no_grad():\n",
        "          Y_pred = model(X_test)\n",
        "          lossVal = loss_fn(Y_pred, Y_test)\n",
        "          lossVal_list[epoch] = lossVal.item()\n",
        "          correct = (torch.argmax(Y_pred, dim=1) == Y_test).type(torch.FloatTensor)\n",
        "          accVal_list[epoch] = correct.mean() # media de predicciones correctas por epoch\n",
        "  return lossTrain_list, accTrain_list, lossVal_list, accVal_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qaVG9wztTHe"
      },
      "outputs": [],
      "source": [
        "import tqdm\n",
        "num_epochs  = 50\n",
        "#convierte datos del tensor a variables del auto-grad\n",
        "X_train = Variable(Xtrain).float()\n",
        "Y_train = Variable(Ytrain).long()\n",
        "X_test  = Variable(Xtest).float()\n",
        "Y_test  = Variable(Ytest).long()\n",
        "\n",
        "lossTrain_list, accTrain_list, lossVal_list, accVal_list = train(model, X_train, Y_train, X_test, Y_test, num_epochs)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h25OMWvozblo"
      },
      "source": [
        "# 4. Visualización de resultados\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gdKRRBmzeVH"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(x=range(num_epochs), y=accVal_list, label=\"Validacion\")\n",
        "sns.lineplot(x=range(num_epochs), y=accTrain_list, label=\"Entrenamiento\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Train versus Validation: Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jx0JB3QcVlQs"
      },
      "outputs": [],
      "source": [
        "sns.lineplot(x=range(num_epochs), y=lossVal_list, label=\"Validacion\")\n",
        "sns.lineplot(x=range(num_epochs), y=lossTrain_list, label=\"Entrenamiento\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Cross Entropy Loss\")\n",
        "plt.title(\"Train versus Validation: Loss \")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW4dmQVlEbVf"
      },
      "source": [
        "¿Ha observado el comportamiento que esperaba? El optimizador SGD tiene el inconveniente de que es necesario hacer una buena selección de los parámetros para que funcione como esperaríamos. Uno de los optimizadores más usados es Adam, ya que se requiere un menor ajuste y tiene una tasa de aprendizaje adaptativa.\n",
        "\n",
        "Repita la prueba pero usando Adam en lugar de SGD con una tasa de aprendizaje baja, como $0.001$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRJTf5ojEazY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uphij3MsFg0Z"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZvdtsfFFtmG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UA5zavInZHHO"
      },
      "source": [
        "## MATRIZ DE CONFUSIÓN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KbTdHpgxx6n"
      },
      "outputs": [],
      "source": [
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgeAwVdIZGrS"
      },
      "outputs": [],
      "source": [
        "from torchmetrics import ConfusionMatrix\n",
        "\n",
        "total = 0\n",
        "correct = 0\n",
        "\n",
        "\n",
        "X_test  = Variable(Xtest).float()\n",
        "Y_test  = Variable(Ytest).long()\n",
        "Y_pred = model(X_test)\n",
        "correct = (torch.argmax(Y_pred, dim=1) == Y_test).type(torch.FloatTensor)\n",
        "confmat = ConfusionMatrix(task=\"multiclass\",num_classes=3)\n",
        "\n",
        "total = Y_test.size()\n",
        "\n",
        "print(f\"Porcentaje de patrones bien clasificados: {100 *correct.mean():.2f}%\")\n",
        "print(\"Matriz de Confusion\")\n",
        "print(confmat(Y_pred, Y_test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOtkh43ednV5"
      },
      "source": [
        "# PRÁCTICAS\n",
        "1. Entrena durante más iteraciones, (100, 150) y visualiza los resultados.\n",
        "2. Compara los resultados con las siguientes opciones:\n",
        "  * Cambiando el número de capas ocultas (1 o 2 capas)\n",
        "  * Cambiando el número de unidades ocultas (25, 100 y 500)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snMHSxwT1yIO"
      },
      "source": [
        "Tenga en cuenta que puede añadir todas las celdas que estime oportunas para poder ejecutar las distintas configuraciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-t1UgwJxC4c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4K_Yt_HP1yIO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybsel5a91yIO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Py311",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}